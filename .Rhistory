break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
print(res)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
new_theta <- theta - (leaning_rate*gradient_glob)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta = theta - (leaning_rate*gradient_glob)/batch_size
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
print(res)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
new_theta <- theta - (leaning_rate*gradient_glob)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta = theta - (leaning_rate*gradient_glob)/batch_size
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
new_theta <- theta - (leaning_rate*gradient_glob)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta = theta - (leaning_rate*gradient_glob)/batch_size
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
gradient_aggr <- apply(gradient_glob,1,sum)
new_theta <- theta - (leaning_rate*gradient_aggr)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta = theta - (leaning_rate*gradient_glob)/batch_size
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
gradient_aggr <- apply(gradient_glob,1,sum)
print(gradient_aggr)
new_theta <- theta - (leaning_rate*gradient_aggr)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta = theta - (leaning_rate*gradient_glob)/batch_size
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
0.072415053  + 0.02303179 +  0.062849393
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
gradient_aggr <- apply(gradient_glob,1,sum)
print(gradient_aggr)
print(length(theta))
print(length(gradient_aggr))
new_theta <- theta - (leaning_rate*gradient_aggr)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta <- new_theta
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, X, y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
# Mini batch
dgs_minibatch_online_parallle_2<- function(X,y,theta, batch_size=100,ncores=3, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle du taux d'apprentissage
if (leaning_rate <= 0){
stop("'learn_rate' doit etre superieur à zero")
}
# Controle de la tolerance
if (tolerance <= 0){
stop("'tolerance' doit etre superieur à zero")
}
# Controle du max iteratons
if (max_iter <= 0){
stop("'max_iter' doit etre superieur à zero")
}
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
iter <- 0
cost_vector = c()
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("theta","sigmoid","gradient"))
start <- 1
#mede = c(0)
while(iter < max_iter){
stop = start + (batch_size-1)
if(stop >= dim(X)[1]){
stop = dim(X)[1]
break
}
xBatch = X[start:stop,]
yBatch = y[start:stop]
xBatch$biais <-  1
df <- cbind(yBatch,xBatch)
data_app = decoupage_ligne(df, ncores)
task <- function(k){
# Sample group for each node
app_X<- data_app[data_app$fold == k, -1]
app_Y <- data_app[data_app$fold == k, 1]
# delete colonne fold
app_X$fold = NULL
app_Y$fold = NULL
grad <- gradient(theta,app_X,as.integer(app_Y))
return(grad)
}
res <- clusterApply(cl, x=1:ncores, task)
gradient_glob = sapply(res, function(x) x)
print(gradient_glob)
gradient_aggr <- apply(gradient_glob,1,sum)
new_theta <- theta - (leaning_rate*gradient_aggr)/batch_size
# Controle de convergence
if (sum(abs(theta-new_theta)) < tolerance){
break
}
theta <- new_theta
clusterExport(cl, "theta")
# Calcul du cout
cost = log_loss(theta, app_X, app_Y)
# Historisation de la fonction de cout
cost_vector = c(cost_vector, cost)
# Gestion iteration
iter <- iter + 1
start <- start + batch_size
}
stopCluster(cl)
return(list(theta_final = theta, history_cost = cost_vector, nbIter=iter))
}
## Gradient descente minibatch &nonline parallele
print(system.time(res <- dgs_minibatch_online_parallle_2(as.data.frame(X)[,-1],y,theta, max_iter = 100, tolerance=1e-07)))
decath <- read.table("decathlon.csv", sep = ";", header = T, row.names=1)
setwd("/Users/macbookair/Desktop/Lyon2/SISE/Fouille de données/data")
decath <- read.table("decathlon.csv", sep = ";", header = T, row.names=1, check.names = F)
View(decath)
set.sedd(123)
# nstart permet de specifier le nombre d'initialisaion aleatoire (repete 100 fois)
# la classification avec la variance intra-classe la plus faible est retenue
class <- kmeans(scale(decath[,1:10]), centers = 4, nstart = 100)
summary(class)
class
class$centers
source('~/Desktop/Lyon2/SISE/Fouille de données/FouilleDonnées.R', echo=TRUE)
setwd("/Users/macbookair/Desktop/Lyon2/SISE/Fouille de données/data")
decath <- read.table("decathlon.csv", sep = ";", header = T, row.names=1, check.names = F)
set.seed(123)
# la classification avec la variance intra-classe la plus faible est retenue
classe <- kmeans(scale(decath[,1:10]), centers = 4, nstart = 100)
classe$centers
classe$withinss
#caracteriser les classes
decath.comp <- cbind.data.frame(decath, classe=factor(classe$cluster))
View(decath.comp)
library(FactoMineR)
catdes(decath.comp,num.var=14)
########## CAH
df <- data.frame(obs =c(0,3,7,10,1,8))
df
library(cluster)
classif<- agnes(df, method = ward)
classif<- agnes(df, method = "ward")
classif
plot(classif, xlab="Individu", which.plot=2, main="Dendrogramme")
############ KMEANS
classe <- kmeans(df, centers = 2, nstart = 100)
classif<- agnes(df, method = "complete")
plot(classif, xlab="Individu", which.plot=2, main="Dendrogramme")
classif
classif$order
classif$ac
classif$height
# comparons Kmean et CAH
table(class$cluster,cutree(classif,k=2))
cutree(classif,k=2)
class$cluster
# comparons Kmean et CAH
table(classe$cluster,cutree(classif,k=2))
library(dgrGlm)
?dgrglm.fit
