#clusterExport(cl, "stat_glob")
#}
stopCluster(cl)
#return(list(theta_final = stat_glob, nbIter=iter))
}
columnSDG_paralle(X,y)
columnSDG_paralle<- function(X,y,ncores=3, batch_size=10, leaning_rate=0.1, max_iter=100, tolerance=1e-06){
# Controle de dimension
if (dim(X)[1] != length(y)){
stop("les dimensions de 'x' et 'y' ne correspondent pas")
}
# Decoupage colonne de mon jeu
data_app = decoupage_colonne(X, ncores)
stat_glob <- c()
isInitLocalTheta <- FALSE
# Init Theta
list_localtheta <- list()
for(i in 1:ncores){
list_localtheta[[i]] <- c()
}
#calcules des statistiques local
calculStats <- function(k){
# Calcul stats local
dataX = data_app[[k]]
grad <- rep(0,ncol(dataX))
if(isInitLocalTheta==FALSE){
localTheta = runif(ncol(dataX))
isInitLocalTheta = TRUE
}
# for saving local stats
local_stats = rep(0,ncol(dataX))
local_stats = list()
i<-1
for (start in seq(from=1, to=dim(dataX)[1], batch_size)){
stop = start + (batch_size-1)
if(stop > dim(dataX)[1]){
break
}
xBatch = dataX[start:stop,]
# compute the partial statistics using local data and localTheta
local_stats <- local_stats + as.vector(xBatch%*%localTheta)
local_stats[[i]] = xBatch%*%localTheta
i <- i+1
if(last_iter == TRUE){
break
}
}
local_stats = dataX%*%localTheta
return(local_stats)
}
# update model in compute the gradient and update the model
updateModel <- function(statAggr,k){
dataX = data_app[[k]]
grad <- rep(0,ncol(dim_local_model))
for (start in seq(from=1, to=dim(dataX)[1], batch_size)){
stop = start + batch_size
if(stop >= dim(dataX)[1]){
stop = dim(dataX)[1]
last_iter = TRUE
}
xBatch <- dataX[start:stop,]
yBatch <- y[start:stop,]
# save gradient
grad = grad+gradient(theta, xBatch, yBatch)
if(last_iter == TRUE){
break
}
}
theta = theta - leaning_rate*grad/batch_size
}
iter <- 0
# Paralellisation du calcul de gradient
cl <- makeCluster(ncores)
clusterExport(cl, c("sigmoid","gradient"))
calculStats <- clusterApply(cl,x=1:ncores,calculStats)
print(lapply(calculStats, function(x) x))
#print(apply(sapply(calculStats, function(x) x),1,sum))
#while(iter < max_iter){
#iter <- iter + 1
# Master function stat1+stat2+....
#stat_glob = apply(sapply(res, function(x) x),1,sum)
#clusterExport(cl, "stat_glob")
#}
stopCluster(cl)
#return(list(theta_final = stat_glob, nbIter=iter))
}
library(parallel)
require(foreach)
require(doParallel)
set.seed(100)
n <- 100
p <- 20
theta = runif(p+1)
X <- cbind(1,matrix(rnorm(n*p),n,p))
X1 = matrix(rnorm(n*p),n,p)
Z <- X %*% theta
fprob <- ifelse(Z<0, exp(Z)/(1+exp(Z)),1/(1+exp(-Z)))
y<- rbinom(n,1,fprob)
data = as.data.frame(cbind(y,X1))
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(Glass)
data<-data(Glass)
View(Glass)
nom_var<-names(data)
nom_var
nom_var<-names(Glass)
nom_var
p<-ncol(Glass)-1
p
n<-dim(Glass)[1]
n
nom_var_exp <- names(Glass[,1:9])
nom_var_exp
# Nom class cible
nom_class_cible <- c("building_fp","building_nfp","vehicule_fp","containers ","tableware","headlamps")
# Statistique des variables
summary(Glass[,1:9])
X=scale(Glass[,1:p])
Y=cbind(ifelse(Glass$Type==1,1,0),ifelse(Glass$Type==2,1,0),ifelse(
Glass$Type==3,1,0),ifelse(Glass$Type==5,1,0),ifelse(Glass$Type==6,1,0),ifelse(Glass$Type==7,1,0)) Y=as.data.frame(Y)
Glass=cbind(X,Y) names(Glass)=c(nom_var_exp ,nom_clas_cib)
# PRETRAITEMENT DES DONNÉES
X=scale(Glass[,1:p])
Y=cbind(ifelse(Glass$Type==1,1,0),ifelse(Glass$Type==2,1,0),ifelse(
Glass$Type==3,1,0),ifelse(Glass$Type==5,1,0),ifelse(Glass$Type==6,1,0),ifelse(Glass$Type==7,1,0))
Y=as.data.frame(Y)
Glass=cbind(X,Y) names(Glass)=c(nom_var_exp ,nom_clas_cib)
names(Glass)=c(nom_var_exp ,nom_clas_cib)
names(Glass)=c(nom_var_exp ,nom_class_cib)
names(Glass)=c(nom_var_exp ,nom_class_cible)
names(Glass)=c(nom_var_exp ,nom_class_cible)
Glass
names(Glass)=c(nom_var_exp ,nom_class_cible)
names(Glass)
names(Glass)=c(nom_var_exp ,nom_class_cible)
Glass=cbind(X,Y)
library(tidyverse)
library(plotROC)
# GENERATION DONNÉES LOGISTIQUE
set.seed(103)
n <-100
p <- 10
theta = runif(p+1) # or theta = runif(7)
X <- cbind(1,matrix(rnorm(n*p),n,p)) #  6 Variables quantitative
X1 = matrix(rnorm(n*p),n,p)
X1 <- as.data.frame(X1)
Z <- X %*% theta # combinaison lineare de variable
fprob <- ifelse(Z<0, exp(Z)/(1+exp(Z)),1/(1+exp(-Z))) # Calcul des probas d'affectation
y<- rbinom(n,1,fprob)
data = as.data.frame(cbind(y,X1))
# SEQUENTIEL
print(system.time(model_batch_seq <- dgrglm.fit(y~., data = data, ncores=3, mode_compute="sequentiel",leaning_rate=0.1, max_iter=2000,tolerance=1e-06)))
library(dgrGlm)
# SEQUENTIEL
print(system.time(model_batch_seq <- dgrglm.fit(y~., data = data, ncores=3, mode_compute="sequentiel",leaning_rate=0.1, max_iter=2000,tolerance=1e-06)))
# PARALLEL
print(system.time(model_batch_parallel <- dgrglm.fit(y~., data = data, ncores=3, mode_compute="parallel",leaning_rate=0.1, max_iter=1000,tolerance=1e-06)))
# EVALUATE PERFORMANCE MODEL
perf <- evaluate_performance(model_batch_seq$probas,model_batch_parallel$probas,model_batch_seq$y_val[,1])
compare_model<- function(probas_mod1, probas_mod2, y){
predProbas <- data.frame(model1=probas_mod1, model2=probas_mod2)
# Estimation des classes en fonctions des probas
predClass <- apply(predProbas >= 0.5, 2, factor, labels=c(0,1))
predClass <- data.frame(predClass)
# Erreur de classification des deux modéles
df_err<- predClass %>%
mutate(obs=y) %>%
summarise_all(funs(err=mean(obs!=.))) %>%
select(-obs_err) %>%
round(3)
# Etude des courbes ROC
df_roc <- predProbas %>%
mutate(obs=y) %>%
gather(key = methode, value=score, model1, model2)
toPlot<- ggplot(df_roc)+
aes(d=obs,m=score, color=methode)+
geom_roc()+
theme_classic()
return(list(predProbas = predProbas, PredClass = predClass, models_error = df_err,  toPlot=toPlot))
}
# EVALUATE PERFORMANCE MODEL
perf <- compare_model(model_batch_seq$probas,model_batch_parallel$probas,model_batch_seq$y_val[,1])
perf
library(xlsx)
data <- read.xlsx(file="~/Desktop/Lyon2/SISE/AtelierMachLeraning/Reg Logistique_opt_hyp/ionosphere.xlsx",sheetIndex=1,header=T)
data = data[,-33]
# SEQUENTIEL
print(system.time(model_batch_seq <- dgrglm.fit(y~., data = data, ncores=3, mode_compute="sequentiel",leaning_rate=0.1, max_iter=2000,tolerance=1e-06)))
# PARALLEL
print(system.time(model_batch_parallel <- dgrglm.fit(y~., data = data, ncores=3, mode_compute="parallel",leaning_rate=0.1, max_iter=1000,tolerance=1e-06)))
# EVALUATE PERFORMANCE MODEL
perf <- compare_model(model_batch_seq$probas,model_batch_parallel$probas,model_batch_seq$y_val[,1])
perf
perf$toPlot
unique(iris$species)
unique(iris$Species)
unique(iris$Species)[1]
class(unique(iris$Species))
for (i in unique(iris)){
print(i)
}
unique(iris)
for (i in unique(iris$Species)){
print(i)
}
df_y <- data.frame()
for (i in unique(iris$Species)){
y_courant<- ifelse(y==i,0,1)
df_y <- cbind(df_y,y_courant)
}
list_y <- list()
j<-1
for (i in unique(iris$Species)){
list_y[[j]]<- ifelse(y==i,0,1)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
list_y
list_y <- list()
j<-1
for (i in unique(iris$Species)){
list_y[[j]]<- ifelse(y==i,1,0)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
list_y
ifelse(iris$Species=='versicolor',1,0)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[j]]<- ifelse(y==i,1,0)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
print(ifelse(y==i,1,0))
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
print(ifelse(iris$Species==i,1,0))
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
as.data.frame(list_y)
cbind(list_y[[1]],list_y[[2]],list_y[[3]])
list_y
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
print(ifelse(iris$Species==i,1,0))
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
cbind(list_y[[1]],list_y[[2]],list_y[[3]])
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[1]]<- ifelse(iris$Species==i,1,0)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
cbind(list_y[[1]],list_y[[2]],list_y[[3]])
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
cbind(list_y[[1]],list_y[[2]],list_y[[3]])
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
#df_y <- cbind(df_y,y_courant)
}
datas<-cbind(list_y[[1]],list_y[[2]],list_y[[3]])
colnames(datas)<-unique(iris$Species)
datas
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
datas<-cbind(list_y[[1]],list_y[[2]],list_y[[3]])
colnames(datas)<-unique(iris$Species)
sapply(list_y,function(x) x)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
sapply(list_y,function(x) x)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
datas<- sapply(list_y,function(x) x)
class(datas)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
print(i)
list_y[[i]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
datas<- as.data.frame(sapply(list_y,function(x) x))
class(datas)
View(datas)
rep(1,2)
rep(runif(ncol(iris),3))
rep(runif(ncol(iris)),3)
list_theta = list()
for(i in 1:length(unique(iris$Species))){
list_theta[[i]] <- runif(ncol(iris))
}
df_theta<- as.data.frame(sapply(list_theta,function(x) x))
View(df_theta)
list_theta = list()
for(i in 1:length(unique(iris$Species))){
list_theta[[i]] <- runif(ncol(iris))
}
df_theta<- as.data.frame(sapply(list_theta,function(x) x))
colnames(df_theta) <- unique(y)
list_theta = list()
for(i in 1:length(unique(iris$Species))){
list_theta[[i]] <- runif(ncol(iris))
}
df_theta<- as.data.frame(sapply(list_theta,function(x) x))
colnames(df_theta) <- unique(iris$Species)
View(df_theta)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
list_y[[j]]<- ifelse(y==i,1,0)
j<-j+1
}
df_modalite<- as.data.frame(sapply(list_y,function(x) x))
View(df_modalite)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
list_y[[j]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
df_modalite<- as.data.frame(sapply(list_y,function(x) x))
View(df_modalite)
list_y <- list()
j<-1
for (i in unique(iris$Species)){
list_y[[j]]<- ifelse(iris$Species==i,1,0)
j<-j+1
}
df_modalite<- as.data.frame(sapply(list_y,function(x) x))
set.seed(seed = 102)
rows <- sample(nrow(df_modalite))
df_modalite <- df_modalite[rows, ]
View(df_modalite)
df = data.frame(1:3,1:3)
df
df[,1]<-c(1,5,6)
df
a = list()
a[[1]] = 2
a[[1]] = append(a[[1]],3)
a
b = list()
b[[1]]
b[[1]] = 3
b
b = list()
b[[1]]=append(b[[1]],1)
b = list(1)
b = list(NA)
b
b[[1]]=append(b[[1]],1)
b
library(dgrGlm)
multi <- dgrglm.multiclass.fit(Species ~., data=iris)
#' @import parallel
#' @importFrom stats formula as.formula runif model.frame
#'
#' @return an instance of model
#' @export
#'
#' @examples
#' \dontrun{
#'  dgrglm.multiclass.fit(formule, data)
#' }
dgrglm.multiclass.fit <- function(formule, data, leaning_rate=0.1, max_iter=100, tolerance=1e-04, random_state=102, centering = FALSE){
# OBJECT S3
instance <- list()
# CONTROL OF USER INPUTS
if(!is.formula(formule)){
stop("formula must be of type formula")
}
if(!is.data.frame(data)){
stop("The data source must be a data frame")
}
if (leaning_rate <= 0){
stop("'learn_rate' must be greater than zero")
}
if (tolerance <= 0){
stop("'tolerance' must be greater than zero")
}
if (max_iter <= 0){
stop("'max_iter' must be greater than zero")
}
# COLUMN MATCHING CONTROL BETWEEN DATA AND FORMULA
f = formula(formule)
colonne_names = colnames(data)
for (v in all.vars(f)){
if(!is.element(v, colonne_names) && v != '.'){
print(paste("Whoops!! Correspondence error between variables:: -->", v))
stop("Check the concordance between the columns of the formula and those of the data source")
}
}
# RECONSITUTE DATAFRAME FROM THE FORMULA
df = model.frame(formula = as.formula(formule), data = data)
# SHUFLE DATA
if(!is.null(random_state)){
set.seed(seed = random_state)
rows <- sample(nrow(df))
df <- df[rows, ]
}
# REMOVE NA in DATASET
df <- na.omit(df)
# SAVE EXPLICATIVES VARIABLE IN INSTANCE
instance$explicatives = colnames(df[,-1])
# TARGET AND EXPLICATIVES VARIABLES
y = df[,1]
X = df[,-1]
# CENTERING AND REDUCTION EXPLICATIVES VARIABLE
if(centering == TRUE){
X = centering.reduction(X)
}
# CREATE BIAIS COLUMN
X$biais = 1
# LIST OF Y BOTH OF BOTH
list_y <- list()
j<-1
for (i in unique(y)){
list_y[[j]]<- ifelse(y==i,1,0)
j<-j+1
}
df_modalite<- as.data.frame(sapply(list_y,function(x) x))
colnames(df_modalite) <- unique(y)
# SHUFLE DATAFRAME MODALITY
if(!is.null(random_state)){
set.seed(seed = random_state)
rows <- sample(nrow(df_modalite))
df_modalite <- df_modalite[rows, ]
}
# DATAFRAME THETA
list_theta = list()
for(i in 1:length(unique(y))){
list_theta[[i]] <- runif(ncol(X))
}
df_theta<- as.data.frame(sapply(list_theta,function(x) x))
colnames(df_theta) <- unique(y)
# CONVERT DATA
X = as.matrix(X)
# LOGISTIC REGRESSION IMPLEMENTATION FOR EACH CLASS
iter <- 1
list_cost <- list(NA)
while(iter < max_iter){
for(i in 1:length(unique(y))){
theta <- df_theta[,i]
y <- df_modalite[,i]
cost = logLoss(theta, as.matrix(X), y)
list_cost[[i]] = append(list_cost[[i]],cost)
grad = gradient(theta, as.matrix(X), y)
new_theta = theta - leaning_rate*grad
df_theta[,i] <- new_theta
}
}
list_cost <- na.omit(list_cost)
# AT THIS LEVEL, WE HAVE THE ESTIMATED COEFFICIENTS IN DF_THETA
instance$history_cost <- list_cost
instance$df_theta <- df_theta
class(instance) <- "modele"
return(instance)
}
multi <- dgrglm.multiclass.fit(Species ~., data=iris)
library(dgrGlm)
multi <- dgrglm.multiclass.fit(Species ~., data=iris)
library(dgrGlm)
multi <- dgrglm.multiclass.fit(Species ~., data=iris)
library(dgrGlm)
multi <- dgrglm.multiclass.fit(Species ~., data=iris)
